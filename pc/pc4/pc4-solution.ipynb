{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PC4 - Sélection de modèle et régularisation - 21 juin 2021 - Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce notebook aborde les notions de régularisation et de sélection de modèles :\n",
    "- La régularisation consiste à imposer des contraintes supplémentaires à votre modèle pour lui éviter de surapprendre sur les données d'entraînement. Ici nous présentons les **régularisations L1 et L2** (partie 1).\n",
    "- La sélection de modèle consiste à élaborer des protocoles d'évaluation robustes pour comparer les performances de différents modèles sur un problème donné. La technique utilisée est celle de la **validation croisée** (partie 2).\n",
    "- Un problème concret sur une base de données réelle permet de mettre en applications les concepts introduits (partie 3).\n",
    "\n",
    "Enfin, de part les exemples présentés, une variante de la régression linéaire est présentée : la **régression polynomiale**. \n",
    "\n",
    "Ce notebook vous est proposé par [Arthur Imbert](https://github.com/Henley13)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce notebook a été créé avec les versions suivantes : \n",
    "```\n",
    "numpy==1.20.2\n",
    "matplotlib==3.4.2\n",
    "pandas==1.2.4\n",
    "sklearn==0.24.2\n",
    "scipy==1.6.3\n",
    "```\n",
    "Des différences de version peuvent expliquer des comportements inattendus (avertissements, messages d'erreurs, fonctionalités inexistantes) mais il n'est pas nécessaire a priori d'avoir exactement les versions listées ci-dessus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check your versions\n",
    "print(\"numpy=={0}\".format(np.__version__))\n",
    "print(\"matplotlib=={0}\".format(matplotlib.__version__))\n",
    "print(\"pandas=={0}\".format(pd.__version__))\n",
    "print(\"sklearn=={0}\".format(sklearn.__version__))\n",
    "print(\"scipy=={0}\".format(scipy.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set font size in plots\n",
    "plt.rc('font', **{'size': 12})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Régularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import r2_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un modèle complexe a plus de chance de surapprendre. C'est le cas par exemple si vous avez un grand nombre de variables ou plusieurs hyperparamètres à optimiser pour faire fonctionner le modèle. En général, la régularisation prend la forme d'un terme additionel dans la fonction de perte qui pénalise les modèles complexes. L'idée est d'introduire volontairement un biais dans le modèle pour le rendre plus robuste. Nous préférons un modèle un peu moins précis sur les données d'entraînement, mais capable de conserver le même niveau de performance sur de nouvelles données de test. \n",
    "\n",
    "Nous allons utiliser dans ce notebook deux méthodes parmi les plus couramment utilisées pour régulariser les modèles linéaires :\n",
    "- [Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) cf. Section 8.5 du poly\n",
    "- [Lasso](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) cf. Section 8.6 du poly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Régularisation L2 (Ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La **régularisation L2** appliquée à une régression linéaire (également appelé **régression Ridge**) consiste à résoudre le problème convexe suivant :\n",
    "\n",
    "$$\n",
    "    \\underset{\\beta \\in \\mathbb{R}^{p+1}}{\\arg\\min} \\lVert y - X\\beta \\rVert^2_2 + \\lambda \\lVert \\beta \\rVert^2_2\n",
    "$$\n",
    "\n",
    "Le terme $\\lVert \\beta \\rVert^2_2$ pénalise les valeurs extrêmes de $\\beta$. Intuitivement nous pouvons voir que cette contrainte va inciter le modèle à ne pas trop se spécialiser \" la solution n'associe pas des poids exagérés à des dimensions de $X$ qui permettrait éventuellement de prédire parfaitement les données d'entraînement. \n",
    "\n",
    "L'hyperparamètre $\\lambda$ (appelé _coefficient de régularisation_) permet d'accorder plus ou moins d'importance à votre terme de régularisation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour illustrer l'impact d'une régularisation Ridge, nous allons simuler un jeu de données non linéaire qui prendra la forme d'une courbe sinusoïdale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_samples = 30\n",
    "\n",
    "# set the random seed to generate the same random numbers\n",
    "np.random.seed(13)\n",
    "\n",
    "# ground truth function such that y = f(X)\n",
    "def f(X):\n",
    "    return np.cos(1.5 * np.pi * X) * 5\n",
    "\n",
    "# compute ground truth distribution\n",
    "X_gt = np.linspace(0, 1, 100).reshape(-1, 1)\n",
    "y_gt = f(X_gt)\n",
    "print(X_gt.shape, y_gt.shape)\n",
    "\n",
    "# simulate random samples from this distribution\n",
    "X = np.sort(np.random.rand(nb_samples, 1))\n",
    "y = f(X)\n",
    "\n",
    "# add noise\n",
    "y += np.random.randn(nb_samples, 1) * 0.3\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize plot frame\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# plot ground truth function f\n",
    "plt.plot(X_gt, y_gt, label=\"True function\", c=\"orange\", linewidth=2)\n",
    "\n",
    "# plot samples\n",
    "plt.scatter(X, y, label=\"Observed samples\", c=\"forestgreen\", marker=\"o\", s=50)\n",
    "\n",
    "# format plot\n",
    "plt.xlabel(\"X\", fontweight=\"bold\", fontsize=15)\n",
    "plt.ylabel(\"y\", fontweight=\"bold\", fontsize=15)\n",
    "plt.xlim((0, 1))\n",
    "plt.ylim((-7, 7))\n",
    "plt.title(\"y = f(X)\", fontweight=\"bold\", fontsize=20)\n",
    "plt.legend(loc=\"best\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous proposons 3 méthodes pour estimer au mieux cette courbe à partir d'une « droite » de régression :\n",
    "- Une régression linéaire\n",
    "- Une régression polynomiale\n",
    "- Une régression polynomiale régularisée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset for train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1 Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question :__ Combien de *features* (variables) avons-nous dans notre problème ?\n",
    "\n",
    "__Réponse :__ Une seule, $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrainons une régression linéaire « classique » (comme celle vue à la PC3) sur `(X_train, y_train)` et évaluons sa performance d'une part sur le jeu d'entraînement et d'autre part sur le jeu de test. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question :__ Pourquoi comparer ces deux performances ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Réponse :__ Si la performance est moins bonne sur le jeu de test, cela indique une situation de sur-apprentissage. À l'inverse, si la performance est meilleure sur le jeu de test, cela indique un sous-apprentissage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "# R2\n",
    "print(\"R2\")\n",
    "r2_score_reg_train = r2_score(y_train, reg.predict(X_train))\n",
    "print(\"\\r train: {0:0.2f}\".format(r2_score_reg_train))\n",
    "\n",
    "r2_score_reg_test = r2_score(y_test, reg.predict(X_test))\n",
    "print(\"\\r test: {0:0.2f}\".format(r2_score_reg_test))\n",
    "\n",
    "# mean squared error\n",
    "print(\"root mean squared error\")\n",
    "rmse_reg_train = mean_squared_error(y_train, reg.predict(X_train), squared=False)\n",
    "print(\"\\r train: {0:0.2f}\".format(rmse_reg_train))\n",
    "\n",
    "rmse_reg_test = mean_squared_error(y_test, reg.predict(X_test), squared=False)\n",
    "print(\"\\r test: {0:0.2f}\".format(rmse_reg_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question :__ Que pouvez-vous conclure sur le choix de la régression linéaire ?\n",
    "\n",
    "__Réponse :__ Le modèle sous-apprend. Les performances sur le jeu de test sont même un peu meilleures (elles restent dans à moins d'un écart-type de celles sur le jeu d'entrainement). Remarquez que la différence entre la performance sur le jeu d'entraînement et sur le jeu de test va dépendre d'où précisément sont situées les observations de chacun des jeux (par exemple, si tous les points du jeu d'entraînement sont entre $x=0.2$ et $x=0.6$, une partie de la sinusoïde qui peut être bien approchée par une droite, et que les points du jeu de test sont à l'extérieur de cet intervalle, la performance sera fortement dégradée sur le jeu de test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpolate regression line\n",
    "y_model = reg.predict(X_gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant afficher le modèle appris sur le graphe précédent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize plot frame\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# plot ground truth function f\n",
    "plt.plot(X_gt, y_gt, '--', label=\"True function\", c=\"orange\", linewidth=2)\n",
    "\n",
    "# plot regression line\n",
    "plt.plot(X_gt, y_model, label=\"Learned function\", c=\"steelblue\", linewidth=2)\n",
    "\n",
    "# plot train and test samples\n",
    "plt.scatter(X_train, y_train, label=\"Train samples\", c=\"steelblue\", marker=\"X\", s=50)\n",
    "plt.scatter(X_test, y_test, label=\"Test samples\", c=\"firebrick\", marker=\"D\", s=50)\n",
    "\n",
    "# format plot\n",
    "plt.xlabel(\"X\", fontweight=\"bold\", fontsize=15)\n",
    "plt.ylabel(\"y\", fontweight=\"bold\", fontsize=15)\n",
    "plt.xlim((0, 1))\n",
    "plt.ylim((-7, 7))\n",
    "plt.title(\"Linear regression\", fontweight=\"bold\", fontsize=20)\n",
    "plt.legend(loc=\"best\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question :__ Le graphique est-il cohérent avec les performances calculées ?\n",
    "\n",
    "__Réponse :__ Oui, une approche purement linéaire a clairement des limites ici."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 Régression polynomiale "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La régression polynomiale consiste à apprendre un modèle non-linéaire en apprenant un modèle linéaire sur un nouvel ensemble de variables, formé de monomes des variables décrivant nos données.\n",
    "\n",
    "De manière générale, pour un problème décrit par $p$ variables $(X_1, X_2, \\dots, X_p)$, une régression polynomiale de degré $d$ est une régression linéaire sur les variables $(X_1, X_2, \\dots, X_p, X_1^2, X_1 X_2, \\dots, X_p^2, \\dots, X_p^d)$. Remarquez que nous créons ainsi un grand nombre de variables, corrélées entre elles ; nous gagnons en finesse de modélisation, mais perdons en complexité du modèle, risque de surapprentissage, et fléau de la dimension. Nous parlerons plus de modèles non-linéaires au chapitre 9. \n",
    "\n",
    "Une telle transformation est possible avec la classe `PolynomialFeatures` de `sklearn.preprocessing`.\n",
    "\n",
    "Ici, il s'agit donc de régresser une droite à partir des puissances de $X$ et non plus de $X$ uniquement. Dans notre exemple, $\\beta$ n'est plus un scalaire mais un vecteur associant un coefficient à $X^1$, $X^2$, $X^3$, ..., $X^{15}$. On approxime $y$ comme un polynôme de $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute polynomial features\n",
    "polynomial_features = PolynomialFeatures(degree=15, include_bias=False)\n",
    "\n",
    "X_train_polynomial = polynomial_features.fit_transform(X_train)\n",
    "X_test_polynomial = polynomial_features.transform(X_test)\n",
    "X_gt_polynomial = polynomial_features.transform(X_gt)\n",
    "\n",
    "print(X_train_polynomial.shape)\n",
    "print(X_test_polynomial.shape)\n",
    "print(X_gt_polynomial.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question :__ Combien de *features* (variables) avons-nous dans notre problème ?\n",
    "\n",
    "__Réponse :__ Il y en a 15 : $X^1$, $X^2$, $X^3$, ..., $X^{15}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train_polynomial, y_train)\n",
    "\n",
    "# R2\n",
    "print(\"R2\")\n",
    "r2_score_reg_train = r2_score(y_train, reg.predict(X_train_polynomial))\n",
    "print(\"\\r train: {0:0.2f}\".format(r2_score_reg_train))\n",
    "r2_score_reg_test = r2_score(y_test, reg.predict(X_test_polynomial))\n",
    "print(\"\\r test: {0:0.2f}\".format(r2_score_reg_test))\n",
    "\n",
    "# root mean squared error\n",
    "print(\"root mean squared error\")\n",
    "rmse_reg_train = mean_squared_error(y_train, reg.predict(X_train_polynomial), squared=False)\n",
    "print(\"\\r train: {0:0.2f}\".format(rmse_reg_train))\n",
    "rmse_reg_test = mean_squared_error(y_test, reg.predict(X_test_polynomial), squared=False)\n",
    "print(\"\\r test: {0:0.2f}\".format(rmse_reg_test))\n",
    "\n",
    "# interpolate regression line\n",
    "y_model = reg.predict(X_gt_polynomial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question :__ Que pouvez-vous conclure sur le choix de la régression polynomiale ?\n",
    "\n",
    "__Réponse :__ Le modèle surapprend. Les performances du jeu d'entraînement sont bien supérieures à celles du jeu de test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant comparer le modèle appris à la vérité terrain (la fonction qui nous a servi à simuler les données)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize plot frame\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# plot ground truth function f\n",
    "plt.plot(X_gt, y_gt, '--', label=\"True function\", c=\"orange\", linewidth=2)\n",
    "\n",
    "# plot regression line\n",
    "plt.plot(X_gt, y_model, label=\"Learned function\", c=\"steelblue\", linewidth=2)\n",
    "\n",
    "# plot train and test samples\n",
    "plt.scatter(X_train, y_train, label=\"Train samples\", c=\"steelblue\", marker=\"X\", s=50)\n",
    "plt.scatter(X_test, y_test, label=\"Test samples\", c=\"firebrick\", marker=\"D\", s=50)\n",
    "\n",
    "# format plot\n",
    "plt.xlabel(\"X\", fontweight=\"bold\", fontsize=15)\n",
    "plt.ylabel(\"y\", fontweight=\"bold\", fontsize=15)\n",
    "plt.xlim((0, 1))\n",
    "plt.ylim((-7, 7))\n",
    "plt.title(\"Polynomial regression\", fontweight=\"bold\", fontsize=20)\n",
    "plt.legend(loc=\"best\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question :__ Le graphique est-il cohérent avec les performances calculées ?\n",
    "\n",
    "__Réponse :__ Oui, la courbe estimée colle complètement aux données d'entraînement, mais pas du tout à la vérité terrain (le  modèle surapprend). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.3 Régression polynomiale régularisée ridge\n",
    "\n",
    "Comme la régression polynomiale surapprend, nous allons maintenant lui appliquer un terme de régularisation ridge pour essayer de compenser cet effet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "ridge = Ridge(alpha=0.01, random_state=13)\n",
    "ridge.fit(X_train_polynomial, y_train)\n",
    "\n",
    "# R2\n",
    "print(\"R2\")\n",
    "r2_score_ridge_train = r2_score(y_train, ridge.predict(X_train_polynomial))\n",
    "print(\"\\r train: {0:0.2f}\".format(r2_score_ridge_train))\n",
    "r2_score_ridge_test = r2_score(y_test, ridge.predict(X_test_polynomial))\n",
    "print(\"\\r test: {0:0.2f}\".format(r2_score_ridge_test))\n",
    "\n",
    "# mean squared error\n",
    "print(\"root mean squared error\")\n",
    "rmse_ridge_train = mean_squared_error(y_train, ridge.predict(X_train_polynomial), squared=False)\n",
    "print(\"\\r train: {0:0.2f}\".format(rmse_ridge_train))\n",
    "rmse_ridge_test = mean_squared_error(y_test, ridge.predict(X_test_polynomial), squared=False)\n",
    "print(\"\\r test: {0:0.2f}\".format(rmse_ridge_test))\n",
    "\n",
    "# interpolate regression line\n",
    "y_model = ridge.predict(X_gt_polynomial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question :__ Que pouvez-vous conclure sur le choix de la régularisation Ridge ?\n",
    "\n",
    "__Réponse :__ Le modèle semble performant. La régression polynomiale permet d'approximer une courbe non linéaire, mais la régularisation évite le risque de surapprentissage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparons le modèle que nous venons d'apprendre à la fonction nous ayant permi de simuler les données :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize plot frame\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# plot ground truth function f\n",
    "plt.plot(X_gt, y_gt, '--', label=\"True function\", c=\"orange\", linewidth=2)\n",
    "\n",
    "# plot regression line\n",
    "plt.plot(X_gt, y_model, label=\"Learned function\", c=\"steelblue\", linewidth=2)\n",
    "\n",
    "# plot train and test samples\n",
    "plt.scatter(X_train, y_train, label=\"Train samples\", c=\"steelblue\", marker=\"X\", s=50)\n",
    "plt.scatter(X_test, y_test, label=\"Test samples\", c=\"firebrick\", marker=\"D\", s=50)\n",
    "\n",
    "# format plot\n",
    "plt.xlabel(\"X\", fontweight=\"bold\", fontsize=15)\n",
    "plt.ylabel(\"y\", fontweight=\"bold\", fontsize=15)\n",
    "plt.xlim((0, 1))\n",
    "plt.ylim((-7, 7))\n",
    "plt.title(\"Polynomial regression + L2 regularization\", fontweight=\"bold\", fontsize=20)\n",
    "plt.legend(loc=\"best\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Remarque :__  Vous pouvez comparer les coefficients du modèle polynomial appris sans régularisation avec les coefficients du modèle appris avec régularisation, et observer que la contrainte sur la norme du vecteur de coefficients de régression a effectivement réduit l'amplitude de ces coefficients :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Coefficients de la régression polynomiale :\", reg.coef_)\n",
    "print(\"Coefficients de la régression polynomiale régularisée :\", ridge.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question :__ Avec la régression polynomiale, était-il possible d'éviter le surapprentissage sans utiliser la régularisation L2 ? \n",
    "\n",
    "__Réponse :__ Un modèle moins complexe aurait pu faire l'affaire. Dans notre cas, une régression polynomiale avec moins de degrés."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Régularization L1 (Lasso)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La **régularisation L1** appliquée à une régression linéaire (également appelé **régression Lasso**) consiste à résoudre le problème suivant :\n",
    "\n",
    "$$\n",
    "    \\underset{\\beta \\in \\mathbb{R}^{p+1}}{\\arg\\min} \\lVert y - X\\beta \\rVert^2_2 + \\lambda \\lVert \\beta \\rVert_1\n",
    "$$\n",
    "\n",
    "L'impact de la pénalisation L1 est plus radical que celui de la pénalisation L2. Le terme $\\lVert \\beta \\rVert_1$ pénalise tous les coefficients non nuls. Cette contrainte va inciter le modèle à mettre à zéro les coefficients des variables les moins influentes. La solution est dite parcimonieuse ou *sparse* (un nombre limité de coefficients non nuls). L'hyperparamètre $\\lambda$ (_coefficient de régularisation_) permet d'accorder plus ou moins d'importance à votre terme de régularisation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour illustrer l'impact d'une régularisation Lasso, nous allons simuler un jeu de données parcimonieux à approximer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_samples = 100\n",
    "nb_features = 200\n",
    "noise_ratio = 0.1\n",
    "\n",
    "# set the random seed to generate the same random numbers\n",
    "np.random.seed(13)\n",
    "\n",
    "# sample random multidimensional input\n",
    "X = np.random.randn(nb_samples, nb_features)\n",
    "\n",
    "# decreasing coefficients with alternated signs for visualization\n",
    "idx = np.arange(nb_features)\n",
    "beta = (-1) ** idx * np.exp(-idx / 10)\n",
    "\n",
    "# sparsify coefficients\n",
    "beta[10:] = 0\n",
    "\n",
    "# target variable\n",
    "y = np.dot(X, beta)\n",
    "\n",
    "# add noise\n",
    "y += np.random.randn(nb_samples) * noise_ratio\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisons notre modèle :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize plot frame\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# plot ground truth coefficient values\n",
    "nonzero_coef_id = np.where(beta)[0]\n",
    "nonzero_coef_value = beta[beta != 0]\n",
    "m, s, _ = plt.stem(idx, beta, \n",
    "                   markerfmt='o', label='True coefficients', use_line_collection=True)\n",
    "plt.setp([m, s], color=\"firebrick\", linewidth=2)\n",
    "\n",
    "# format plot\n",
    "plt.xlabel(\"Coefficients\", fontweight=\"bold\", fontsize=15)\n",
    "plt.ylabel(\"Coefficient value\", fontweight=\"bold\", fontsize=15)\n",
    "plt.title(\"Sparse dataset\", fontweight=\"bold\", fontsize=20)\n",
    "plt.legend(loc=\"best\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question :__ Quelle est la proportion de coefficients non nuls utilisées pour simuler les données ?\n",
    "\n",
    "__Réponse :__ Seulement 5% des coefficients sont non nuls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici la difficulté pour l'apprentissage vient de ce qu'une grande partie de notre jeu de données n'a pas d'influence sur notre variable d'intérêt $y$. A nouveau, nous commençons par entraîner une régression linéaire afin de comparer les résultats avec le Lasso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset for train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Régression linéaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "# R2\n",
    "print(\"R2\")\n",
    "r2_score_reg_train = r2_score(y_train, reg.predict(X_train))\n",
    "print(\"\\r train: {0:0.2f}\".format(r2_score_reg_train))\n",
    "r2_score_reg_test = r2_score(y_test, reg.predict(X_test))\n",
    "print(\"\\r test: {0:0.2f}\".format(r2_score_reg_test))\n",
    "\n",
    "# mean squared error\n",
    "print(\"root mean squared error\")\n",
    "rmse_reg_train = mean_squared_error(y_train, reg.predict(X_train), squared=False)\n",
    "print(\"\\r train: {0:0.2f}\".format(rmse_reg_train))\n",
    "rmse_reg_test = mean_squared_error(y_test, reg.predict(X_test), squared=False)\n",
    "print(\"\\r test: {0:0.2f}\".format(rmse_reg_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question :__ Que pouvez-vous conclure sur le choix de la régression linéaire ?\n",
    "\n",
    "__Réponse :__ Le modèle surapprend. Les performances sur le jeu d'entraînement sont bien supérieures à celles sur jeu de test. Ce n'est pas surprenant : avec un nombre de variables supérieur au nombre d'observations, on peut apprendre un modèle qui colle parfaitement aux données (système de n équations linéaires à p inconnues)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 Régression linéaire régularisée Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Code :__ Entrainez une régression Lasso (paramètre de régularisation `alpha=0.1`) sur les données d'entraînement. Reportez-vous à la documentation : https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html (l'API est similaire à celle de la régularisation ridge).\n",
    "\n",
    "Calculez le R2 ajusté et la *RMSE* sur les données d'entraînement et de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solution\n",
    "\n",
    "# train model\n",
    "lasso = Lasso(alpha=0.1, random_state=13)\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "# R2\n",
    "print(\"R2\")\n",
    "r2_score_lasso_train = r2_score(y_train, lasso.predict(X_train))\n",
    "print(\"\\r train: {0:0.2f}\".format(r2_score_lasso_train))\n",
    "r2_score_lasso_test = r2_score(y_test, lasso.predict(X_test))\n",
    "print(\"\\r test: {0:0.2f}\".format(r2_score_lasso_test))\n",
    "\n",
    "# mean squared error\n",
    "print(\"root mean squared error\")\n",
    "rmse_lasso_train = mean_squared_error(y_train, lasso.predict(X_train), squared=False)\n",
    "print(\"\\r train: {0:0.2f}\".format(rmse_lasso_train))\n",
    "rmse_lasso_test = mean_squared_error(y_test, lasso.predict(X_test), squared=False)\n",
    "print(\"\\r test: {0:0.2f}\".format(rmse_lasso_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question :__ Que pouvez-vous conclure sur le choix du Lasso ?\n",
    "\n",
    "__Réponse :__ Le modèle semble performant, avec une erreur similaire sur le jeu d'entrainement et le jeu de test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant visualiser les coefficients du modèle appris par le Lasso : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize plot frame\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# plot ground truth coefficient values\n",
    "nonzero_coef_id = np.where(beta)[0]\n",
    "nonzero_coef_value = beta[beta != 0]\n",
    "m, s, _ = plt.stem(nonzero_coef_id, nonzero_coef_value, \n",
    "                   markerfmt='o', label='True coefficients', use_line_collection=True)\n",
    "plt.setp([m, s], color=\"firebrick\", linewidth=2)\n",
    "\n",
    "# plot linear coefficient values\n",
    "nonzero_coef_id = np.where(reg.coef_)[0]\n",
    "nonzero_coef_value = reg.coef_[reg.coef_ != 0]\n",
    "m, s, _ = plt.stem(nonzero_coef_id, nonzero_coef_value, \n",
    "                   markerfmt='D', label='Linear coefficients', use_line_collection=True)\n",
    "plt.setp([m, s], color=\"steelblue\", linewidth=2)\n",
    "\n",
    "# plot Lasso coefficient values\n",
    "nonzero_coef_id = np.where(lasso.coef_)[0]\n",
    "nonzero_coef_value = lasso.coef_[lasso.coef_ != 0]\n",
    "m, s, _ = plt.stem(nonzero_coef_id, nonzero_coef_value, \n",
    "                   markerfmt='X', label='Lasso coefficients', use_line_collection=True)\n",
    "plt.setp([m, s], color=\"forestgreen\", linewidth=2)\n",
    "\n",
    "# format plot\n",
    "plt.xlabel(\"Coefficients\", fontweight=\"bold\", fontsize=15)\n",
    "plt.ylabel(\"Estimated value\", fontweight=\"bold\", fontsize=15)\n",
    "title_str = \"Linear $R^2$: {0:0.2f} | Lasso $R^2$: {01:0.2f}\".format(r2_score_reg_test, r2_score_lasso_test)\n",
    "plt.title(title_str, fontweight=\"bold\", fontsize=20)\n",
    "plt.legend(loc=\"best\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question :__ Combien de variables sont utilisées par le modèle lasso ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Réponse :__ 10, comme dans le modèle originale (valeur accessible par `len(nonzero_coef_id)`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question :__ Comment expliquer les mauvaises performances de la régression linéaire (non régularisée) ?\n",
    "\n",
    "__Réponse :__ Avec moins d'observations que de variables, il n'y pas de solution unique pour le problème que résout l'équation linéaire. La solution finale accorde trop de poids à des variables sans aucune influence sur notre variable d'intérêt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sélection de modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_validate, GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre objectif est de sélectionner le meilleur modèle pour un problème donné. Pour comparer différent modèles nous utilisons un ou plusieurs critères de performance sur un jeu de données que l'on aurait exclue dès le début de l'analyse (le jeu de test).\n",
    "\n",
    "En particulier, nous avons utilisé une valeur fixe du coefficient de régularisation (`alpha` dans `scikit-learn`, $\\lambda$ dans le poly). Rien ne garantit que cette valeur soit optimale.\n",
    "\n",
    "Jusqu'à maintenant, séparer les observations en jeu d'entraînement et de test suffisait à entraîner et évaluer les modèles. Cependant, pour des problèmes plus complexes nécessitant des modèles plus élaborées, une préparation des données en amont et un traitement particulier des résultats en aval, il est très facile de surapprendre sur les données de test elles-mêmes ! Vous risquez d'optimiser votre chaîne d'analyse (le coefficient $\\lambda$ de la régularisation, ou plus généralement toute valeur d'hyperparamètre ; le choix des variables à conserver ; etc.) afin de maximiser les performances sur un jeu de test qui n'est plus totalement inconnu. Une solution est de séparer vos observations initiales en trois jeux de données : **entraînement** (pour entraîner les modèles), **validation** (pour sélectionner le meilleur modèle et optimiser ses hyperparamètres) et **test** (pour évaluer).\n",
    "\n",
    "La **validation croisée** permet d'aller plus loin et de répéter les séparations *entraînement/test* ou *entraînement/validation/test* à partir du même jeu d'observations. Les mesures de performances moyennées sur ces différentes séparations sont plus robustes. \n",
    "\n",
    "Reportez-vous à la section 8.2 du poly pour ces notions. \n",
    "\n",
    "`sklearn` propose différentes variantes de la validation croisée :\n",
    "- [K-fold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html) : la validation croisée telle que nous l'avons vue en cours et telle que décrite dans le poly\n",
    "- [Stratified K-fold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html) : même chose, mais pour les problèmes de classification, en faisant en sorte que la proportion d'exemples de chaque classe soit respectée dans chaque bloc/fold (_stratification_)\n",
    "\n",
    "(Pour aller plus loin)\n",
    "- [Leave-One-Out](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LeaveOneOut.html) : autant de folds que d'échantillons ; requiert donc d'entrainer $n$ modèles (ce qui est coûteux en ressources) et a l'inconvénient de produire des modèles très similaire les uns aux autres (leurs jeux d'entrainement différant d'une seule observation) qui, étant testés sur une observation à la fois, peuvent donner une grande variance dans l'estimation de l'erreur de généralisation. Elle est ainsi peu utilisée, sauf cas particulier.\n",
    "- [Leave-P-Out](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LeavePOut.html) : chaque sous ensemble de taille $P$ du jeu de données est utilisé comme jeu de test (et le reste comme jeu d'entrainement) ; on obtient ainsi un grand nombre de modèles (ce qui peut être très coûteux en ressources) et des jeux de tests non disjoints (contrairement à la validation croisée). Relativement peu utilisée aussi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour la suite, nous utilisons le même jeu de données parcimonieux que pour le Lasso. La validation croisée permet une évaluation plus robuste de notre modèle, mais également d'optimiser le coefficient de régularisation $\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pour générer de nouveau le jeu de données si besoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_samples = 100\n",
    "nb_features = 200\n",
    "noise_ratio = 0.1\n",
    "\n",
    "# set the random seed to generate the same random numbers\n",
    "np.random.seed(13)\n",
    "\n",
    "# sample random multidimensional input\n",
    "X = np.random.randn(nb_samples, nb_features)\n",
    "\n",
    "# decreasing coefficients with alternated signs for visualization\n",
    "idx = np.arange(nb_features)\n",
    "beta = (-1) ** idx * np.exp(-idx / 10)\n",
    "\n",
    "# sparsify coefficients\n",
    "beta[10:] = 0\n",
    "\n",
    "# target variable\n",
    "y = np.dot(X, beta)\n",
    "\n",
    "# add noise\n",
    "y += np.random.randn(nb_samples) * noise_ratio\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note :** \n",
    "\n",
    "Pour clarifier cette section, il y a deux méthodes distinctes qui sont présentées et parfois mélangées ici :\n",
    "- La **validation croisée** qui permet de mesurer la performance de modèles sur plusieurs *folds* et donc de calculer des performances moyennes sur plusieurs séparations de données. L'objectif étant d'avoir une mesure de la performance robuste à l'aléa de la séparation. Il s'agit d'éviter d'obtenir de bonnes ou de mauvaises performances \"par chance\", parce que vous auriez séparé un jeu de test facile ou non représentatif. Répéter l'opération via une validation croisée permet de se prémunir de ce biais potentiel.\n",
    "- La **séparation du jeu de données** en trois (train/validation/test) et non plus en deux (train/test). Pour analyser vos données, vous allez entraîner un modèle, mesurer sa performance (avec une validation croisée ou non) et modifier votre modèle (paramètre, préprocessing, postprocessing) afin d'améliorer votre performance. En itérant de la sorte vous risquez de surapprendre sur les données de test que vous utilisez pour mesurer la performance. Vous modifiez par itération votre *pipeline* pour maximiser les performances sur vos données de test. Cela peut être contre-productif et nuire aux performances de votre *pipeline* sur une nouvelle base de test que vous n'auriez jamais vue. Pour cette raison, il peut être judicieux d'utiliser une base intermédiaire de validation pour optimiser votre modèle et de conserver des données de test pour évaluer vos performances. Formulé autrement, les décisions que vous prenez sur votre *pipeline* (ses paramètres et son architecture) doivent être complètement indépendantes des données que vous utilisez pour évaluer votre modèle. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Validation croisée"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En utilisant `sklearn.model_selection.train_test_split` et `sklearn.model_selection.KFold` nous isolons un jeu de test dès le début, puis nous tirons aléatoirement K *folds* d'entraînement et de validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define different train-validation-test splits\n",
    "X_train_, X_test, y_train_, y_test = train_test_split(X, y, test_size=0.2, random_state=13)\n",
    "print(\"nb_test: {1}\".format(len(X_train_), len(X_test)), \"\\n\")\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=13)\n",
    "\n",
    "# loop over the different folds\n",
    "scores = []\n",
    "for i, (train_index, validation_index) in enumerate(kf.split(X_train_)):\n",
    "    print(\"fold: {0} | nb_train: {1} | nb_validation: {2}\".format(i, len(train_index), len(validation_index)))\n",
    "\n",
    "    # get train and validation dataset\n",
    "    X_train = X_train_[train_index]\n",
    "    X_validation = X_train_[validation_index]\n",
    "    y_train = y_train_[train_index]\n",
    "    y_validation = y_train_[validation_index]\n",
    "    \n",
    "    # preprocess data\n",
    "    # ...\n",
    "    \n",
    "    # train model\n",
    "    lasso = Lasso(alpha=0.1)\n",
    "    lasso.fit(X_train, y_train)\n",
    "\n",
    "    # postprocess results\n",
    "    # ...\n",
    "    \n",
    "    # measure performance (here, R2)\n",
    "    r2_score_lasso_validation = r2_score(y_validation, lasso.predict(X_validation))\n",
    "    scores.append(r2_score_lasso_validation)\n",
    "    print(\"\\r R2 (validation): {0:0.3f}\".format(r2_score_lasso_validation))\n",
    "    \n",
    "print()\n",
    "\n",
    "# performance on the validation set\n",
    "average_score = np.mean(scores)\n",
    "std_score = np.std(scores)\n",
    "print(\"Average R2 (validation): {0:0.3f}\".format(average_score))\n",
    "print(\"Standard deviation of R2 (validation): {0:0.3f}\".format(std_score), \"\\n\")\n",
    "\n",
    "# train a new model with both the train and validation dataset\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X_train_, y_train_)\n",
    "\n",
    "# evaluate performance on the test set\n",
    "r2_score_lasso_test = r2_score(y_test, lasso.predict(X_test))\n",
    "print(\"R2 (test): {0:0.3f}\".format(r2_score_lasso_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plus simplement, on peut directement calculer une métrique de performance en moyennant plusieurs folds avec `sklearn.model_selection.cross_validate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a model\n",
    "lasso = Lasso(alpha=0.1, random_state=13, max_iter=10000)\n",
    "\n",
    "# compute an average R2 score over 5 different train-test folds\n",
    "cv_results = cross_validate(lasso, X, y, cv=5, return_train_score=True, scoring=\"r2\")\n",
    "print(\"Average R2 (test): {0:0.3f}\".format(np.mean(cv_results[\"test_score\"])))\n",
    "print(\"Standard deviation R2 (test): {0:0.3f}\".format(np.std(cv_results[\"test_score\"])))\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note :**  Comme expliqué dans la note plus haut, deux concepts sont mélangés ici :\n",
    "- Les fonctions `sklearn.model_selection.KFold` et `sklearn.model_selection.cross_validate` permettent d'effectuer une validation croisée en séparant les données en plusieurs *folds* train/test. La seule différence c'est que `sklearn.model_selection.KFold` ne vous renvoie que les identifiants des observations pour chaque *fold*. C'est à vous de récupérer les données en question, d'entraîner le modèle et de calculer les scores de performance. À l'inverse `sklearn.model_selection.cross_validate` entraîne et évalue un modèle sur plusieurs *folds* automatiquement. \n",
    "- Si vous isolez un jeu de test au préalable et que vous utilisez la validation croisée de `sklearn.model_selection.KFold` et `sklearn.model_selection.cross_validate` sur votre jeu de données restant (ici appelé `train_`), vous effectuez de fait une séparation en trois de vos données (train et validation via la validation croisée, ainsi qu'un jeu de test unique isolé dès le début). \n",
    "\n",
    "Dans l'exemple avec `sklearn.model_selection.KFold` nous procédons à une validation croisée avec le schéma train/validation/test parce que nous isolons un jeu de test dès le début. \n",
    "\n",
    "Dans l'exemple avec `sklearn.model_selection.cross_validate` nous appliquons la validation croisée sur l'ensemble de notre jeu de données, donc c'est une validation croisée avec le schéma train/test seulement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question :__ Selon vous, qu'est-ce qui pourrait expliquer la différence de performance entre les méthodes présentées (la première avec `sklearn.model_selection.KFold` et la seconde avec `sklearn.model_selection.cross_validate`) ?\n",
    "\n",
    "__Réponse :__ Si le nombre de *folds* est le même, leur taille ne l'est pas exactement. En utilisant `sklearn.model_selection.KFold` nous avons séparé un jeu de test au préalable. Donc nous faisons 5 *folds* sur 80 observations. En utilisant `sklearn.model_selection.cross_validate` nous faisons les 5 *folds* sur les 100 observations. L'aléatoire de la séparation a normalement été fixé avec les *random seeds*. Si nous simulons moins de 100 observations, l'impact de la taille du *fold* s'accentue !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Code :__ Utilisez `sklearn.model_selection.KFold` pour créer 5 *folds* de validation croisée (c'est-à-dire 5 paires (jeu d'entraînement, jeu de test)), et utilisez-les pour obtenir une valeur moyenne et un écart-type pour le R2 d'un Lasso, de façon à reproduire le comportement de `cross_validate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n",
    "# define different train-test splits\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=13)\n",
    "\n",
    "# loop over the different folds\n",
    "scores = []\n",
    "for i, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "    print(\"fold: {0} | nb_train: {1} | nb_test: {2}\".format(i, len(train_index), len(test_index)))\n",
    "\n",
    "    # get train and validation dataset\n",
    "    X_train = X[train_index]\n",
    "    X_test = X[test_index]\n",
    "    y_train = y[train_index]\n",
    "    y_test = y[test_index]\n",
    "    \n",
    "    # preprocess data\n",
    "    # ...\n",
    "    \n",
    "    # train model\n",
    "    lasso = Lasso(alpha=0.1)\n",
    "    lasso.fit(X_train, y_train)\n",
    "\n",
    "    # postprocess results\n",
    "    # ...\n",
    "    \n",
    "    # measure performance (here, R2)\n",
    "    r2_score_lasso_test = r2_score(y_test, lasso.predict(X_test))\n",
    "    scores.append(r2_score_lasso_test)\n",
    "    print(\"\\r R2 (test): {0:0.3f}\".format(r2_score_lasso_test))\n",
    "    \n",
    "print()\n",
    "\n",
    "# performance on the test set\n",
    "average_score = np.mean(scores)\n",
    "std_score = np.std(scores)\n",
    "print(\"Average R2 (test): {0:0.3f}\".format(average_score))\n",
    "print(\"Standard deviation R2 (test): {0:0.3f}\".format(std_score), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note :** Dans l'exemple ci-dessus, nous ne faisons que reproduire le premier exemple avec `sklearn.model_selection.KFold`, mais sans isoler un jeu de test au préalable. Nous avons bien une validation croisée avec le schéma de séparation train/test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 *Gridsearch*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le **_gridsearch_** consiste à évaluer les performances pour différentes valeurs de paramètres sur différents *folds* de validation croisée afin de retourner le paramètre optimal. La fonction d'usage est `sklearn.model_selection.GridSearchCV`. Dans notre exemple il s'agit d'optimiser le paramètre `alpha` du modèle Lasso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note :**  C'est dans le cadre d'un gridsearch que la séparation en trois (train/validation/test) est particulièrement recommandée. Vous isolez un jeu de test dès le début, puis appliquez un gridsearch sur le jeu de données restant. De cette manière, l'optimisation de vos paramètres reste indépendante des données de test que vous utilisez pour évaluer votre modèle à la fin. __Attention__, ce n'est pas ce que nous faisons dans l'exemple ci-dessous, où le gridsearch est directement appliqué sur l'ensemble des données disponibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model and alpha values to evaluate\n",
    "lasso = Lasso(random_state=13, max_iter=10000)\n",
    "alphas = np.logspace(-4, -1, 30)\n",
    "\n",
    "# define gridsearch\n",
    "tuned_parameters = [{'alpha': alphas}]\n",
    "nb_folds = 5\n",
    "grid = GridSearchCV(lasso, tuned_parameters, cv=nb_folds, refit=False, verbose=3)\n",
    "\n",
    "# run gridsearch \n",
    "grid.fit(X, y)\n",
    "\n",
    "# get R2 (default score with Lasso models)\n",
    "scores = grid.cv_results_['mean_test_score']\n",
    "scores_std = grid.cv_results_['std_test_score']\n",
    "\n",
    "# compute standard errors\n",
    "std_error = scores_std / np.sqrt(nb_folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Remarques :__\n",
    "* le paramètre `verbose=3` permet d'avoir une sortie très détaillée. Vous pouvez l'enlever (ou utiliser d'autres valeurs parmi [0, 1, 2]) pour réduire la quantité de détails.\n",
    "* le paramètre `refit=True` assure que `grid.best_estimator_` contient un modèle d'hyperparamètre(s) optimal, entrainé sur l'ensemble des données passées à `grid.fit`.\n",
    "* __Attention :__ Ici, nous n'avons pas précisé quelle métrique utiliser pour quantifier la performance des différents modèles. Elle se cache dans la méthode `score` de l'objet `lasso`. D'après la [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html?highlight=lasso#sklearn.linear_model.Lasso.score), il s'agit donc du R2. Pour utiliser une autre métrique, il faut utiliser l'argument [`scoring`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html?highlight=gridsearchcv#sklearn.model_selection.GridSearchCV) de `GridSearchCV`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Code :__ A partir des résultats du *gridsearch*, récupérez la valeur `alpha` optimale ainsi que la performance maximale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution\n",
    "\n",
    "# get optimal alpha\n",
    "i_max = np.argmax(scores)\n",
    "best_alpha = alphas[i_max]\n",
    "best_score = scores[i_max]\n",
    "print(\"optimal alpha: {0:0.4f}\".format(best_alpha))\n",
    "print(\"best R2 (test set): {0:0.4f}\".format(best_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution alternative\n",
    "best_alpha = grid.best_params_['alpha']\n",
    "best_score = grid.best_score_\n",
    "print(\"optimal alpha: {0:0.4f}\".format(best_alpha))\n",
    "print(\"best R2 (test set): {0:0.4f}\".format(best_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize plot frame\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# plot scores with a log scale in x-axis\n",
    "plt.semilogx(alphas, scores, label=\"Average R2\")\n",
    "plt.semilogx(alphas, scores + std_error, 'b--')\n",
    "plt.semilogx(alphas, scores - std_error, 'b--')\n",
    "\n",
    "# control the translucency of the fill color with alpha=0.2\n",
    "plt.fill_between(alphas, scores + std_error, scores - std_error, alpha=0.2)\n",
    "\n",
    "# plot best score\n",
    "plt.axhline(np.max(scores), linestyle=':', color=\"firebrick\", label=\"Best R2\")\n",
    "\n",
    "# format plot\n",
    "plt.xlabel(\"alpha\", fontweight=\"bold\", fontsize=15)\n",
    "plt.ylabel(\"CV score +/- std error\", fontweight=\"bold\", fontsize=15)\n",
    "plt.xlim([alphas[0], alphas[-1]])\n",
    "plt.title(\"Gridsearch results (score = R2)\", fontweight=\"bold\", fontsize=20)\n",
    "plt.legend(loc=\"best\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question :__ Que voudrait dire une valeur de `alpha` proche de 0 ? Une valeur proche de 1 ?\n",
    "\n",
    "__Réponse :__ Respectivement, une regression linéaire sans régularisation et une régularisation extrême avec tous les coefficients à 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A ce stade, vous pouvez entraîner votre modèle optimal en utilisant ensemble vos données d'entraînement et de validation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cas pratique (bonus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour ce cas pratique nous utilisons des données cliniques. L'objectif est de **prédire le niveau d'antigène prostatique spécifique** (a.k.a. *PSA* pour *Prostate-Specific Antigen*). C'est une protéine produite exclusivement par la prostate. Un taux de concentration élevé de cette molécule dans le sang est souvent le signe chez l'homme d'un cancer de la prostate. Cet indicateur permet ainsi de suivre l'évolution du cancer.\n",
    "\n",
    "Plus précisément, nous allons essayer de prédire le niveau de concentration du *PSA* (`lpsa`, en échelle logarithmique) à partir des mesures cliniques suivantes :\n",
    "- `cavol` : Le volume de la tumeur (échelle logarithmique).\n",
    "- `lweight` : Le poids de la prostate (échelle logarithmique).\n",
    "- `age`: L'âge du patient.\n",
    "- `lbph`: Le volume de l'hypertrophie bénigne de la prostate (a.k.a. *BPH* pour *Benign Prostatic Hyperplasia*) qui correspond au volume non cancéreux de l'organe (échelle logarithmique).\n",
    "- `svi`: Indicateur sur le fait que le cancer s'est propagé aux vésicules séminales (deux glandes associées à la prostate).\n",
    "- `lcp`: La *pénétration capsulaire* qui mesure à quel point la capsule prostatique (la membrane qui entoure la prostate), a été envahi par le cancer (échelle logarithmique).\n",
    "- `gleason`: Le score *Gleason*. Ce score est établi par un histopathologiste après observation d'une biopsie de la prostate. Pour plus d'information vous pouvez consulter ce lien : http://www.wikiwand.com/en/Gleason_grading_system. \n",
    "* `pgg45`: Le pourcentage de la tumeur qui est accrédité d'un score *Gleason* de 4 ou 5.\n",
    "\n",
    "Ce jeu de données est un jeu de données classique, que l'on trouve par exemple [sur Kaggle](https://www.kaggle.com/tvscitechtalk/prostatecsv). Il est issu de Stamey, T.A., Kabalin, J.N., McNeal, J.E., Johnstone, I.M., Freiha, F., Redwine, E.A. and Yang, N. (1989). Prostate specific antigen in the diagnosis and treatment of adenocarcinoma of the prostate: II. radical prostatectomy treated patients, _Journal of Urology_ 141(5), 1076–1083.\n",
    "\n",
    "\n",
    "Nous avons bien ici un problème de **régression**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "path = \"data/prostate.csv\"\n",
    "df = pd.read_csv(path, index_col=0)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.loc[:, [\"lcavol\", \"lweight\", \"age\", \"lbph\", \"svi\", \"lcp\", \"gleason\", \"pgg45\"]].to_numpy()\n",
    "y = df.loc[:, \"lpsa\"].to_numpy()\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset for train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=13)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalised data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_normalized = scaler.transform(X_train)\n",
    "X_test_normalized = scaler.transform(X_test)\n",
    "print(X_train_normalized.shape)\n",
    "print(X_test_normalized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a model\n",
    "reg = LinearRegression()\n",
    "\n",
    "# compute an average MSE over 5 different train-test folds\n",
    "cv_results = cross_validate(reg, X_train_normalized, y_train, cv=5, return_train_score=True, \n",
    "                            scoring=\"neg_mean_squared_error\")\n",
    "\n",
    "# get MSE\n",
    "print(\"Mean Squared Error (validation set): {0:0.3f}\".format(-np.mean(cv_results[\"test_score\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Recherche du meilleur modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 Linear regression + Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "ridge = Ridge(random_state=13, max_iter=10000)\n",
    "alphas = np.logspace(-4, 4, 30)\n",
    "\n",
    "# define gridsearch\n",
    "tuned_parameters = [{'alpha': alphas}]\n",
    "nb_folds = 5\n",
    "grid = GridSearchCV(ridge, tuned_parameters, cv=nb_folds, refit=True,\n",
    "                    scoring=\"neg_mean_squared_error\")\n",
    "\n",
    "# run gridsearch\n",
    "grid.fit(X_train_normalized, y_train)\n",
    "\n",
    "# get NMSE\n",
    "scores_nmse = grid.cv_results_['mean_test_score']\n",
    "scores_std = grid.cv_results_['std_test_score']\n",
    "\n",
    "# compute standard errors \n",
    "std_error = scores_std / np.sqrt(nb_folds)\n",
    "\n",
    "# get optimal alpha\n",
    "i_max = np.argmax(scores_nmse)\n",
    "\n",
    "# Transform NMSE into MSE\n",
    "scores_mse = -scores_nmse\n",
    "\n",
    "# Attention : ici on reste avec la MSE plutôt que la RMSE car sklearn nous a donné les écarts-types sur la MSE \n",
    "# et la transformation de ces écarts-types en écarts-types de la RMSE n'est pas triviale.\n",
    "best_alpha = alphas[i_max]\n",
    "best_score = scores_mse[i_max]\n",
    "best_score_std = scores_std[i_max]\n",
    "\n",
    "print(\"optimal alpha: {0:0.4f}\".format(best_alpha))\n",
    "print(\"best MSE (validation set): {0:0.4f} +/- {1:0.4f}\".format(best_score, best_score_std))\n",
    "print(\"best RMSE (validation set): {0:0.3f}\".format(np.sqrt(best_score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize plot frame\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# plot scores with a log scale in x-axis\n",
    "plt.semilogx(alphas, scores_mse, label=\"Average MSE\")\n",
    "plt.semilogx(alphas, (scores_mse + std_error), 'b--')\n",
    "plt.semilogx(alphas, (scores_mse - std_error), 'b--')\n",
    "\n",
    "# control the transparency of the fill color with alpha=0.2\n",
    "plt.fill_between(alphas, (scores_mse + std_error), (scores_mse - std_error), alpha=0.2)\n",
    "\n",
    "# plot best score\n",
    "plt.axhline(np.min(scores_mse), linestyle=':', color=\"firebrick\", label=\"Best MSE\")\n",
    "\n",
    "# format plot\n",
    "plt.xlabel(r\"$\\alpha$\", fontweight=\"bold\", fontsize=15)\n",
    "plt.ylabel(\"CV score +/- std error\", fontweight=\"bold\", fontsize=15)\n",
    "plt.xlim([alphas[0], alphas[-1]])\n",
    "plt.title(\"Gridsearch results (score = MSE)\", fontweight=\"bold\", fontsize=20)\n",
    "plt.legend(loc=\"best\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coefficients du modèle\n",
    "\n",
    "Le paramètre `refit=True` assure que `grid.best_estimator_` contient un modèle de régression ridge, d'hyperparamètre optimal, entrainé sur l'ensemble du jeu de données passé à `grid.fit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize plot frame\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# plot best estimator coefficients\n",
    "m, s, _ = plt.stem(np.arange(len(grid.best_estimator_.coef_)), grid.best_estimator_.coef_, \n",
    "                   markerfmt='o', label='Ridge', use_line_collection=True)\n",
    "plt.setp([m, s], linewidth=2)\n",
    "\n",
    "# format plot\n",
    "plt.xlabel(\"Variables\", fontsize=15)\n",
    "plt.xticks(np.arange(len(grid.best_estimator_.coef_)), labels=list(df.columns[:-1]))\n",
    "\n",
    "plt.ylabel(\"Coefficient de régression\", fontsize=15)\n",
    "plt.title(\"Ridge optimisée en validation croisée\", fontsize=20)\n",
    "plt.legend(loc=\"best\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 Linear regression + Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "lasso = Lasso(random_state=15, max_iter=10000)\n",
    "alphas = np.logspace(-4, 0, 30)\n",
    "\n",
    "# define gridsearch\n",
    "tuned_parameters = [{'alpha': alphas}]\n",
    "nb_folds = 5\n",
    "grid = GridSearchCV(lasso, tuned_parameters, cv=nb_folds, refit=True,\n",
    "                    scoring=\"neg_mean_squared_error\")\n",
    "\n",
    "# run gridsearch\n",
    "grid.fit(X_train_normalized, y_train)\n",
    "\n",
    "# get NMSE\n",
    "scores_nmse = grid.cv_results_['mean_test_score']\n",
    "scores_std = grid.cv_results_['std_test_score']\n",
    "\n",
    "# compute standard errors\n",
    "std_error = scores_std / np.sqrt(nb_folds)\n",
    "\n",
    "# get optimal alpha\n",
    "i_max = np.argmax(scores_nmse)\n",
    "\n",
    "# Transform NMSE into MSE\n",
    "scores_mse = -scores_nmse\n",
    "\n",
    "best_alpha = alphas[i_max]\n",
    "best_score = scores_mse[i_max]\n",
    "best_score_std = scores_std[i_max]\n",
    "\n",
    "# Alternative :\n",
    "#i_max = grid.best_index_\n",
    "#best_alpha = grid.best_params['alpha']\n",
    "#best_score = - grid.best_score_\n",
    "#best_score_std = scores_std[i_max]\n",
    "print(\"optimal alpha: {0:0.4f}\".format(best_alpha))\n",
    "print(\"best MSE (validation set): {0:0.4f} +/- {1:0.4f}\".format(best_score, best_score_std))\n",
    "print(\"best RMSE (validation set): {0:0.3f}\".format(np.sqrt(best_score)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize plot frame\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# plot scores with a log scale in x-axis\n",
    "plt.semilogx(alphas, scores_mse, label=\"Average MSE\")\n",
    "plt.semilogx(alphas, (scores_mse + std_error), 'b--')\n",
    "plt.semilogx(alphas, (scores_mse - std_error), 'b--')\n",
    "\n",
    "# control the transparency of the fill color with alpha=0.2\n",
    "plt.fill_between(alphas, (scores_mse + std_error), (scores_mse - std_error), alpha=0.2)\n",
    "\n",
    "# plot best score\n",
    "plt.axhline(min(scores_mse), linestyle=':', color=\"firebrick\", label=\"Best MSE\")\n",
    "\n",
    "# format plot\n",
    "plt.xlabel(r\"$\\alpha$\", fontweight=\"bold\", fontsize=15)\n",
    "plt.ylabel(\"CV score +/- std error\", fontweight=\"bold\", fontsize=15)\n",
    "plt.xlim([alphas[0], alphas[-1]])\n",
    "plt.title(\"Gridsearch results (score = MSE)\", fontweight=\"bold\", fontsize=20)\n",
    "plt.legend(loc=\"best\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Coefficients du modèle\n",
    "\n",
    "Le paramètre `refit=True` assure que `grid.best_estimator_` contient un Lasso, d'hyperparamètre optimal, entrainé sur l'ensemble du jeu de données passé à `grid.fit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize plot frame\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# plot best estimator coefficients\n",
    "m, s, _ = plt.stem(np.arange(len(grid.best_estimator_.coef_)), grid.best_estimator_.coef_, \n",
    "                   markerfmt='o', label='Lasso', use_line_collection=True)\n",
    "plt.setp([m, s], linewidth=2)\n",
    "\n",
    "# format plot\n",
    "plt.xlabel(\"Variables\", fontsize=15)\n",
    "plt.xticks(np.arange(len(grid.best_estimator_.coef_)), labels=list(df.columns[:-1]))\n",
    "\n",
    "plt.ylabel(\"Coefficient de régression\", fontsize=15)\n",
    "plt.title(\"Lasso optimisé en validation croisée\", fontsize=20)\n",
    "plt.legend(loc=\"best\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question :__ Quel est le meilleur modèle prédictif ? Quelles sont les variables les plus utiles pour la prédiction du PSA ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Réponse :__ Un lasso, avec `alpha=0.3`. Les variables les plus importantes sont `lcavol` et `lweight`, suivies de `svi` et `age` puis `gleason`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
